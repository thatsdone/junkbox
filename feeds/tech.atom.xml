<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>That's Done! - tech</title><link href="https://thatsdone.github.io/junkbox/" rel="alternate"></link><link href="https://thatsdone.github.io/junkbox/feeds/tech.atom.xml" rel="self"></link><id>https://thatsdone.github.io/junkbox/</id><updated>2020-01-14T00:01:00+09:00</updated><subtitle>thatsdone's (mostly technical) memorandum</subtitle><entry><title>[ja] Methodologies of Systems Performance - Brendan Gregg</title><link href="https://thatsdone.github.io/junkbox/methodologies_of_systems_performance.html" rel="alternate"></link><published>2020-01-14T00:01:00+09:00</published><updated>2020-01-14T00:01:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2020-01-14:/junkbox/methodologies_of_systems_performance.html</id><summary type="html">&lt;p&gt;Methodologies of Peformance Analysis&lt;/p&gt;</summary><content type="html">&lt;p&gt;Brendan Gregg の "&lt;a href="https://www.amazon.co.jp/dp/0133390098"&gt;Systems Performance: Enterprise and the Cloud&lt;/a&gt;" という本があります。
(翻訳は&lt;a href="https://www.amazon.co.jp/dp/4873117909/"&gt;こちら&lt;/a&gt;) 言わずと知れた DTrace の作者が書いた本で、
全体的な考え方から、CPU・メモリ…等と、具体的な観点ごとに包括的かつ詳細に解説されているので非常によい本です。
軽い勉強会みたいなやつで使おうと思って、まずは方法論(Methodology)が書いてある2章のまとめを作っています。
…が、書いてみたら、結局ほぼ目次に沿って説明するのがいいのかなー…という結論に（笑）&lt;/p&gt;
&lt;p&gt;なお、昨年末に"&lt;a href="https://www.amazon.co.jp/dp/0136554822/"&gt;BPF Performance Tools&lt;/a&gt;"という新しい本も出ていて、
私は発送待ちで待たされていて、いつ日本に来るんだろう？と思っているところだったりします。(苦笑)
まあ、eBPF/XDPについては
&lt;a href="https://www.amazon.co.jp/Linux-Observability-Bpf-Programming-Performance/dp/1492050202/"&gt;これ&lt;/a&gt;
も読んだので、まあぼちぼちでいいっちゃいいのですが。(笑)&lt;/p&gt;
&lt;h2&gt;一番大事なところ - 方法論の章(2章)の構成&lt;/h2&gt;
&lt;p&gt;方法論(Methodology)は2章になります。
この章だけでけっこうな分量になる、まずは、全体を俯瞰するという意味で、目次構成を眺めるだけでも意味があるでしょう。
下のほうに目次の構成をそのまま引用します。少し長いですが、落ち着いて眺めてみると頭が整理できると思います。&lt;/p&gt;
&lt;p&gt;なお、著者本人は、冒頭でこの章をざっくりまとめると以下の３点であると書いています。
2.5節の具体的な方法論と、これらを使うときに使う具体的な Metric の２点が大事なのであって、
他はみんなこの２点に対する背景知識なのだというわけです。&lt;/p&gt;
&lt;p&gt;1) Background
2) Methodology
3) Metrics&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Methodology&lt;ol&gt;
&lt;li&gt;Terminology&lt;ul&gt;
&lt;li&gt;まずは用語定義&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Models&lt;ol&gt;
&lt;li&gt;Systems Under Test&lt;ul&gt;
&lt;li&gt;評価・分析するシステム自体のモデル化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Queueing System&lt;ul&gt;
&lt;li&gt;待ち行列理論 = 数理モデルの基本&lt;/li&gt;
&lt;li&gt;ちなみにこんな本もありますね。&lt;a href="https://www.amazon.co.jp/Performance-Modeling-Design-Computer-Systems/dp/1107027500"&gt;Performance Modeling and Design of Computer Systems: Queueing Theory in Action&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Concepts ＃ 基本概念&lt;ol&gt;
&lt;li&gt;Latency&lt;ul&gt;
&lt;li&gt;一言で「通信遅延」と言っても測定する場所によって、例えばコネクション接続+データ転送+処理時間…等分解されるのに注意。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Time-Scales&lt;ul&gt;
&lt;li&gt;CPUの命令サイクル、メモリアクセス、DISKアクセス…等ものによってかかる時間のスケールが違うのに注意。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Trade-offs&lt;ul&gt;
&lt;li&gt;例えば、I/OサイズとI/O性能・I/Oパターンの関係等、両立しないものの間のトレードオフを知ること。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tuning Efforts&lt;ul&gt;
&lt;li&gt;アプリ～MW～OS～ハードのどのレイヤで性能チューニングを施すのか？一般論としては(アプリに近い)上のレイヤで実施するほど効果は高い。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Level of Appropriateness&lt;ul&gt;
&lt;li&gt;性能分析、チューニングにどこまでコストをかけて深堀りするのかは組織としての投資対効果による。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Point-in-Time Recommendations&lt;ul&gt;
&lt;li&gt;チューニングパラメータの推奨値は、条件によって変わるものなので「その時点のもの」(point-in-time)と思うべし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Load versus Architecture&lt;ul&gt;
&lt;li&gt;システムの性能に影響するのは個々のソフトの「設定値」だけでなく、システムの構造（アーキテクチャ）に由来することもある。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scalability&lt;ul&gt;
&lt;li&gt;スケーラビリティ＝負荷量に対する性能値のふるまい。ふるまいの変化は、スループットの場合はリニアな伸びが変わるところ、レスポンスは一定値（≒リニア）から徐々に劣化が始まるところで見える。典型的な理由は、(なんらかの)リソースの utilization が 100%に達すること(saturation)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Known-Unknowns&lt;ul&gt;
&lt;li&gt;いわゆるknown-known/known-unknown/unknown-unknown。性能評価を進めるにしたがって、unknown-unknownに気づいていくのに注意。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Metrics&lt;ul&gt;
&lt;li&gt;実際に見る具体的な指標。例としては、IOPS、troughput、utilization、latenc等々。忘れてはいけない大事な点として、1) metricの採取自体にもコストがかかること、2) metricの定義や実装そのものに信頼がないようなケースもあること。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Utilization&lt;ul&gt;
&lt;li&gt;利用率には「時間(time)ベース」と「容量(capacity)ベース」の２つがある。前者は単位時間あたりに仕事をしていた時間(busy率)で、後者は処理可能な容量に対する割合。特に前者のbusy率の場合、リソースによっては多重処理が可能なケースもあるので、utilization が100%であっても限界とは限らないのに注意が必要。&lt;/li&gt;
&lt;li&gt;cloud (というか、仮想化)環境の場合には、non-idle time という見方をしたほうがよいという考え方もある。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Saturation &lt;ul&gt;
&lt;li&gt;saturation (飽和)状態とは、処理可能な量を超えて、どの程度のリクエスト(仕事)量が流入しているかを示す程度。処理しきれないリクエストは、待ち行列につながることになる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Profiling&lt;ul&gt;
&lt;li&gt;一般論としては、対象を研究して理解できるような描像を得ること。実際のシステムの Profiling では、典型的にはサンプリングが基本であって、(サンプル間隔しだいだが)得られる描像は荒い（疎な）ものになる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Caching&lt;ul&gt;
&lt;li&gt;xxx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Perspectives&lt;ul&gt;
&lt;li&gt;性能分析には、大きく、1. Resource Analysis と 2. Workload Analysis ２つの側面がある。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Methodology&lt;ol&gt;
&lt;li&gt;Streetlight Anti-Method&lt;/li&gt;
&lt;li&gt;Random Change Anti-Method&lt;/li&gt;
&lt;li&gt;Blame-Someone-Else Anti-Method&lt;/li&gt;
&lt;li&gt;Ad Hoc Checklist Method&lt;/li&gt;
&lt;li&gt;Problem Statement&lt;/li&gt;
&lt;li&gt;Scientific Method&lt;/li&gt;
&lt;li&gt;Diagnosis Cycle&lt;/li&gt;
&lt;li&gt;Tools Method&lt;/li&gt;
&lt;li&gt;The USE Method&lt;ul&gt;
&lt;li&gt;リソースの状況を見るときに Utilization, Saturation, Error の3点をチェックするという有名なやつ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Workload Characterization&lt;/li&gt;
&lt;li&gt;Drill-Down Analysis&lt;ul&gt;
&lt;li&gt;Five Whys&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Latency Analysis&lt;/li&gt;
&lt;li&gt;Method R (by Oracle)&lt;/li&gt;
&lt;li&gt;Event Tracing&lt;/li&gt;
&lt;li&gt;Baseline Statistics&lt;/li&gt;
&lt;li&gt;Static Peformance Tuning&lt;/li&gt;
&lt;li&gt;Cache Tuning&lt;/li&gt;
&lt;li&gt;Micro-Benchmarking&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Modeling&lt;/li&gt;
&lt;li&gt;Enterprise versus Cloud&lt;/li&gt;
&lt;li&gt;Visual Identification&lt;/li&gt;
&lt;li&gt;Linear scalability&lt;/li&gt;
&lt;li&gt;Contention&lt;/li&gt;
&lt;li&gt;Coherence&lt;/li&gt;
&lt;li&gt;Knee point&lt;/li&gt;
&lt;li&gt;Scalability Ceiling&lt;/li&gt;
&lt;li&gt;Amdahl's Law of Scalability&lt;/li&gt;
&lt;li&gt;C(N) = N/(1 + α(N-1))  (C: relative capacity)&lt;/li&gt;
&lt;li&gt;Universal Scalability Law&lt;/li&gt;
&lt;li&gt;C(N) = N/(α(N-1) + βN(N-1))&lt;/li&gt;
&lt;li&gt;Queueing Theory &lt;/li&gt;
&lt;li&gt;M/M/1, M/M/c, M/G/1, M/D/1&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Capacity Planning&lt;ol&gt;
&lt;li&gt;Resource Limits&lt;/li&gt;
&lt;li&gt;Factor Analysis&lt;/li&gt;
&lt;li&gt;Scaling Solutions&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Statistics&lt;ol&gt;
&lt;li&gt;Quantifying Performance&lt;/li&gt;
&lt;li&gt;Averages&lt;/li&gt;
&lt;li&gt;Arithmetric, Geometric, Harmonic, Averages over Time, Decayed&lt;/li&gt;
&lt;li&gt;Standard Deviationos, Percentiles, Median&lt;/li&gt;
&lt;li&gt;Coefficient of Variation&lt;/li&gt;
&lt;li&gt;Multimodal Distributions&lt;/li&gt;
&lt;li&gt;Outliers&lt;ul&gt;
&lt;li&gt;Anomaly???&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Monitoring&lt;ol&gt;
&lt;li&gt;Time-Based Patterns&lt;/li&gt;
&lt;li&gt;Monitoring Products&lt;/li&gt;
&lt;li&gt;Summary-since Boot&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Visualizations&lt;ol&gt;
&lt;li&gt;Line Chart&lt;/li&gt;
&lt;li&gt;Scatter Plots&lt;/li&gt;
&lt;li&gt;Heat Maps&lt;/li&gt;
&lt;li&gt;Surface Plot&lt;/li&gt;
&lt;li&gt;Visualization Tools&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Exercises&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;性能解析の方法論&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Terminology&lt;/li&gt;
&lt;li&gt;Models&lt;ol&gt;
&lt;li&gt;Systems Under Test&lt;/li&gt;
&lt;li&gt;Queueing System&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Concepts
    性能解析で必要になる概念を列挙して説明しています。
    レイテンシ、スループット&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Latency&lt;/li&gt;
&lt;li&gt;Time-Scales&lt;/li&gt;
&lt;li&gt;Trade-offs&lt;/li&gt;
&lt;li&gt;Tuning Efforts&lt;/li&gt;
&lt;li&gt;Level of Appropriateness&lt;/li&gt;
&lt;li&gt;Point-in Time Recommendations&lt;/li&gt;
&lt;li&gt;Load versus Architecture&lt;/li&gt;
&lt;li&gt;Scalability&lt;/li&gt;
&lt;li&gt;Known-Unknowns&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Metrics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Saturation&lt;/li&gt;
&lt;li&gt;Profiling&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perspectives&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resource Analysis&lt;/li&gt;
&lt;li&gt;Workload Analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Methodology&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Streetlight Anti-Method&lt;/li&gt;
&lt;li&gt;Random Change Anti-Method&lt;/li&gt;
&lt;li&gt;Blame-Someone-Else Anti-Method&lt;/li&gt;
&lt;li&gt;Ad Hoc Checklist Method&lt;/li&gt;
&lt;li&gt;Problem Statement&lt;/li&gt;
&lt;li&gt;Scientific Method&lt;/li&gt;
&lt;li&gt;Diagnosis Cycle&lt;/li&gt;
&lt;li&gt;Tools Method&lt;/li&gt;
&lt;li&gt;The USE Method&lt;/li&gt;
&lt;li&gt;Workload Characterization&lt;/li&gt;
&lt;li&gt;Drill-Down Analysis&lt;/li&gt;
&lt;li&gt;Five Whys&lt;/li&gt;
&lt;li&gt;Latency Analysis&lt;/li&gt;
&lt;li&gt;Method R (by Oracle)&lt;/li&gt;
&lt;li&gt;Event Tracing&lt;/li&gt;
&lt;li&gt;Baseline Statistics&lt;/li&gt;
&lt;li&gt;Static Peformance Tuning&lt;/li&gt;
&lt;li&gt;Cache Tuning&lt;/li&gt;
&lt;li&gt;Micro-Benchmarking&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Modeling&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Enterprise versus Cloud&lt;/li&gt;
&lt;li&gt;Visual Identification&lt;/li&gt;
&lt;li&gt;Linear scalability&lt;/li&gt;
&lt;li&gt;Contention&lt;ol&gt;
&lt;li&gt;Coherence&lt;/li&gt;
&lt;li&gt;Knee point&lt;/li&gt;
&lt;li&gt;Scalability Ceiling&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Amdahl's Law of Scalability&lt;/li&gt;
&lt;li&gt;Universal Scalability Law&lt;/li&gt;
&lt;li&gt;C(N) = N/1 + α(N-1) ??? (C: relative capacity)&lt;/li&gt;
&lt;li&gt;Queueing Theory★&lt;/li&gt;
&lt;li&gt;M/M/1, M/M/c, M/G/1, M/D/1&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Capacity Planning★&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resource Limits&lt;/li&gt;
&lt;li&gt;Factor Analysis&lt;/li&gt;
&lt;li&gt;Scaling Solutions&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Statistics&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Quantifying Performance&lt;/li&gt;
&lt;li&gt;Averages&lt;/li&gt;
&lt;li&gt;Arithmetric, Geometric, Harmonic, Averages over Time, Decayed&lt;/li&gt;
&lt;li&gt;Standard Deviationos, Percentiles, Median&lt;/li&gt;
&lt;li&gt;Coefficient of Variation&lt;/li&gt;
&lt;li&gt;Multimodal Distributions&lt;/li&gt;
&lt;li&gt;Outliers&lt;ul&gt;
&lt;li&gt;Anomaly???&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monitoring&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time-Based Patterns&lt;/li&gt;
&lt;li&gt;Monitoring Products&lt;/li&gt;
&lt;li&gt;Summary-since Boot&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visualizations&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Line Chart&lt;/li&gt;
&lt;li&gt;Scatter Plots&lt;/li&gt;
&lt;li&gt;Heat Maps&lt;/li&gt;
&lt;li&gt;Surface Plot&lt;/li&gt;
&lt;li&gt;Visualization Tools&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exercises&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Case Study : むかーし、Redis がなにかも知らなかった頃の話&lt;/h2&gt;</content><category term="tech"></category></entry><entry><title>[ja] back to basics - SRECon19 EMEA revisited, and beyond</title><link href="https://thatsdone.github.io/junkbox/backtobasics_srecon19emea_revisited.html" rel="alternate"></link><published>2020-01-13T12:21:00+09:00</published><updated>2020-01-13T12:21:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2020-01-13:/junkbox/backtobasics_srecon19emea_revisited.html</id><summary type="html">&lt;p&gt;SRE Methodologies&lt;/p&gt;</summary><content type="html">&lt;p&gt;さて、SRECon (や、USENIX LISA)では、ここしばらくCore Principles トラックという形で、
SREのチームの作り方やSREチームの運営のしかた…等といった話を多数聞くことができるわけですが、
&lt;a href="https://thatsdone.github.io/junkbox/srecon19emea.html"&gt;前に書いた記事&lt;/a&gt;で少し触れたように、
&lt;a href="https://www.usenix.org/conference/srecon19emea/"&gt;SRECon19EMEA&lt;/a&gt;の特徴だった、
運用まわりで使える確立した方法論の話をもう一回振り返ってみます。&lt;/p&gt;
&lt;h2&gt;まえおき&lt;/h2&gt;
&lt;p&gt;この記事はあと３回更新します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;まずはポインタの一覧だけ(DONE)&lt;/li&gt;
&lt;li&gt;それぞれ解説(みたいなの)を追加&lt;/li&gt;
&lt;li&gt;SRECon の本筋の Core Princilple の話を追加&lt;/li&gt;
&lt;li&gt;日本発のback to basicな話の事例ということでTPSの話(これは別記事にするかも)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;運用まわりで使える back to basic な方法論&lt;/h2&gt;
&lt;p&gt;とりあえず、私が現地で聞いたセッションの中で気づいたものの一覧だけ。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Systems Theory -システム理論&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E5%88%B6%E5%BE%A1%E7%90%86%E8%AB%96"&gt;wikipedia(ja)&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/leveson"&gt;SRECon19EMEA セッション&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Speaker : Prof. Leveson, MIT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Control Theory - 制御理論&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E5%88%B6%E5%BE%A1%E7%90%86%E8%AB%96"&gt;wikipedia(ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/hahn"&gt;SRECon19EMEA セッション&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Speaker : Ted Hahn, TCB Technologies, and Mark Hahn, Ciber Global&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Incident Command System - 現場指揮システム&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E3%82%A4%E3%83%B3%E3%82%B7%E3%83%87%E3%83%B3%E3%83%88%E3%83%BB%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%83%BB%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0"&gt;wikipedia(ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/hidalgo"&gt;SRECon19EMEA セッション&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Speaker : Alex Hidalgo and Alex Lee, Squarespace&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fault Tree Analysis - 故障木解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E3%83%84%E3%83%AA%E3%83%BC%E8%A7%A3%E6%9E%90"&gt;wikipedia(ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/falko"&gt;SRECon19EMEA セッション&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Speaker : Andrey Falko, Lyft&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Formal Verification Method - 形式的検証手法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%A4%9C%E8%A8%BC"&gt;wikipedia(ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/khlaaf"&gt;SRECon19EMEA セッション&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Speaker : Heidy Khlaaf, Adelard LLP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="tech"></category></entry><entry><title>[ja] SRECon19 EMEA</title><link href="https://thatsdone.github.io/junkbox/srecon19emea.html" rel="alternate"></link><published>2019-10-03T14:30:00+09:00</published><updated>2019-10-03T14:30:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2019-10-03:/junkbox/srecon19emea.html</id><summary type="html">&lt;p&gt;SRECon19EMEA&lt;/p&gt;</summary><content type="html">&lt;p&gt;アイルランドのダブリンで開催中の &lt;a href="https://www.usenix.org/conference/srecon19emea/"&gt;SRECon19EMEA&lt;/a&gt; に来ています。&lt;/p&gt;
&lt;p&gt;ダブリンは初めてで、気温をみて寒そう…とは思っていたのですが、10月頭でもう冬みたいな恰好をしている人が多いとは想像しておらず、
正直、外は寒いです(苦笑)&lt;/p&gt;
&lt;p&gt;初日で一番面白かったのはGoogleの Todd Underwood さんの以下のセッションでしょうか。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.usenix.org/conference/srecon19emea/presentation/underwood"&gt;All of Our ML Ideas Are Bad (and We Should Feel Bad)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MLを使って運用をラクにしたい…というのはけっこう誰でも考える話で、適用対象としては例えば以下のようなものがあるでしょう&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Categorizing/Prioritizing Tickets&lt;/li&gt;
&lt;li&gt;Root Cause Analysis&lt;/li&gt;
&lt;li&gt;Canary Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;しかし、実際に使ってみると...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(MLよりも)もっといい方法ある&lt;/li&gt;
&lt;li&gt;データが足りない&lt;/li&gt;
&lt;li&gt;ラベル付きのデータが足りない&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;といった理由でうまくいかないことがよくあります。&lt;/p&gt;
&lt;p&gt;Todd さんは、これは流行りのやり方を、深く考えずにそのまま使おうとするからだ…と言います。
よく言われる ML というのは、Marketing ML だと。&lt;/p&gt;
&lt;p&gt;Todd さんのおすすめはこんなふうにまとめられます。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Think&lt;ul&gt;
&lt;li&gt;あなたのアプリ、インフラ、お客さんをよく知る人に話を聞きましょう。&lt;/li&gt;
&lt;li&gt;そして、それらを理解するためのアイデアを出すためにブレストをしましょう。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gather&lt;ul&gt;
&lt;li&gt;まずはデータを一か所にあつめる。&lt;/li&gt;
&lt;li&gt;整理してみないとなにもできないので、とにかく整理する。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn&lt;ul&gt;
&lt;li&gt;まずは基本を押さえましょう。&lt;/li&gt;
&lt;li&gt;MLワークロードを動かすプラットフォームは既にたくさんあるので、こういうものを知りましょう。&lt;/li&gt;
&lt;li&gt;そして、集めたデータを使ってプロトタイプをやってみましょう。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;もっと先まで踏み込む場合は&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ontology&lt;/li&gt;
&lt;li&gt;Epistemology&lt;/li&gt;
&lt;li&gt;Metaphysics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の３点を挙げていました。用語は難しいですが、一言で言うと「自分の頭で本質をよく考えろ」ってことですね。 :)
具体的にこうしたらうまくいっているよ…という事例紹介ではありませんが、考え方として、私にはとても参考になったtalkでした。&lt;/p&gt;
&lt;p&gt;Day1 だけでも他にもいろいろ興味深い講演がありました。
例えば、Network trouble の解析の紹介で、よく聞いてみたら eBPF でがんばってCloud Providerにも認めさせたぜ...という
「よくある」話だったりとか(笑)
ぼちぼち更新しようかと思います。&lt;/p&gt;
&lt;p&gt;Stay Tuned ! :)&lt;/p&gt;
&lt;p&gt;2019/10/04 (Dublin現地時間)追記&lt;/p&gt;
&lt;p&gt;SRECon19EMEA、無事に終了しました。
ハリケーンが来ていてアイルランド直撃が予測されいたのですが、大した影響なく済んでよかったです。&lt;/p&gt;
&lt;p&gt;closing での主催者の話によると、今回の出席者は約700名とのこと。3月のNorth Americaでは4桁に届くでしょうか。&lt;/p&gt;
&lt;p&gt;3日間終わってみての所感は、初日のkeynoteのLeverson教授の Systems Theory からはじまり、
Control Theory (制御理論)、Fault Tree Analysis といった、数十年前に確立された理論を振り返る話が多かったことです。
果ては一番最後のセッションが Formal Verification Method...ということで、Program Committee の心意気を感じたように思います。&lt;/p&gt;
&lt;p&gt;私は主に Core Principle の Track に出ていました。具体的にこの手法でうまくいったよ…という話もあることはありましたが、
むしろ Silver Bullet はない。利用者のことをよく知り、自分の頭で考えるべし。ついてはその指針は…という話を多く聞くことができました。
現状の私には非常に参考になる話でした。&lt;/p&gt;
&lt;p&gt;3月の SRECon '20 North America はスケジュールの都合で出席できないのですが、来年度もどれかのSREConに顔を出したいと思います。 :)&lt;/p&gt;</content><category term="tech"></category></entry><entry><title>[ja] BGP in the Data Center</title><link href="https://thatsdone.github.io/junkbox/bgp_in_the_datacenter.html" rel="alternate"></link><published>2019-09-21T20:00:00+09:00</published><updated>2019-09-21T20:00:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2019-09-21:/junkbox/bgp_in_the_datacenter.html</id><summary type="html">&lt;p&gt;BGP in the Data Center&lt;/p&gt;</summary><content type="html">&lt;p&gt;一部で話題の O'reily の "BGP in the Data Center" を読んだ。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[https://www.oreilly.com/library/view/bgp-in-the/9781491983416/(https://www.oreilly.com/library/view/bgp-in-the/9781491983416/)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これは面白い…というか、自分が今やってること的にとても勉強になったのでメモ。&lt;/p&gt;
&lt;p&gt;日本語の他の方の記事もけっこうあるようだ。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://foobaron.hatenablog.com/entry/bgp-in-the-data-center-01"&gt;https://foobaron.hatenablog.com/entry/bgp-in-the-data-center-01&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.bobuhiro11.net/2019/03-21-bgp-in-the-data-center.html"&gt;https://blog.bobuhiro11.net/2019/03-21-bgp-in-the-data-center.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;遡ると'14の JANOG33 で Microsoft の人が講演していたりとか。(時期はこの本よりも早い)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.janog.gr.jp/meeting/janog33/program/bgp.html"&gt;https://www.janog.gr.jp/meeting/janog33/program/bgp.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;1. Introduction to Data Center Networks&lt;/h3&gt;
&lt;p&gt;物理構成の話。L3 Leaf-Spine 型のNWの構成のベストプラクティスの解説。&lt;/p&gt;
&lt;h3&gt;2. How BGP Has Benn Adapted to the Data Center&lt;/h3&gt;
&lt;p&gt;物理構成ができたら、その上の routing をどうやるのか？の話。BGP話。&lt;/p&gt;
&lt;p&gt;しかし、BGPの Best Path Algorithm が参照する metric が8つあって、それを覚えるのにこんなのがあるとは知らなかったw
Cisco(当時?)の Denise Fishburne さんが考えたらしい。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;mnemonic&lt;/th&gt;
&lt;th&gt;-&lt;/th&gt;
&lt;th&gt;BGP metric&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Wise&lt;/td&gt;
&lt;td&gt;W&lt;/td&gt;
&lt;td&gt;Weight&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lisp&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;LOCAL_PREFERENCE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lovers&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;Locally Originated&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apply&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;AS_PATH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Oral&lt;/td&gt;
&lt;td&gt;O&lt;/td&gt;
&lt;td&gt;ORIGIN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Medication&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;MED&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Every&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;eBGP over iBGP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Night&lt;/td&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;NextHop IGP Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(うー、Pelican の調整がまだ不完全で border が出ないな...)&lt;/p&gt;
&lt;h3&gt;3. Building an Automatable BGP Configuration&lt;/h3&gt;
&lt;p&gt;各router (というか、TOR/Leaf/Spine等スイッチ)の BGP 設定は、
愚直に書くと機器ごとに個別の設定や、個別の機器内でも重複した記述が多数出てくるわけだが、
これを自動化のためにカイゼンできるか？の解説。&lt;/p&gt;
&lt;p&gt;Spine側のASNの割り当てはちょっと誤解していたところがあった...&lt;/p&gt;
&lt;h3&gt;4. Reimaging BGP Configuration&lt;/h3&gt;
&lt;p&gt;3章の解説でもろもろ設定を単純化したとして、まだ以下のような pain point がある。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NW機器間(というか、ホストも含むかな...)のインターフェースに付与して管理するのは大変。&lt;/li&gt;
&lt;li&gt;隣(neighbour)のASの定義をいちいち書くのも大変。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一つ目の問題の解決として、(IPv6) Un-Numbered の解説。&lt;/p&gt;
&lt;p&gt;二つめの問題の解決として、remote-as 構文の拡張の external / internal を使う方法の解説。&lt;/p&gt;
&lt;p&gt;ちな、僕が読んだバージョンの p58 の Figure 4-2 の下半分の図は 10.1.1.0/24 じゃなくて、10.1.2.0/24 だと思う。&lt;/p&gt;
&lt;h3&gt;5. BGP Life Cycle Mamagement&lt;/h3&gt;
&lt;p&gt;状態表示でよく使うコマンドのほか、メンテのために停止させたい時の常套手段と、デバッグのやりかた。&lt;/p&gt;
&lt;p&gt;デバッグ用の機能を使って、経路計算のロジックを勉強する…というのもあるのか。&lt;/p&gt;
&lt;h3&gt;6. BGP on the Host&lt;/h3&gt;
&lt;p&gt;この構成で本格的に動かす場合、ホストも BGP をしゃべることになるわけで、
今までのSEとNEという境界もあいまいになるよね…という話。&lt;/p&gt;
&lt;p&gt;そのほか、Anycast 話とか、Dynamic Neighbour とか。&lt;/p&gt;
&lt;p&gt;そういえば、題材に使われている &lt;a href="https://frrouting.org/"&gt;FRR&lt;/a&gt; を使い込む…
というタスクもあったのだが、再開せねば...(とおいめ&lt;/p&gt;</content><category term="tech"></category></entry><entry><title>[ja] Stray Sheep - OpenShift and CloudFoundry</title><link href="https://thatsdone.github.io/junkbox/openshift_and_cloudfoundry.html" rel="alternate"></link><published>2019-09-14T15:01:00+09:00</published><updated>2019-09-14T15:01:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2019-09-14:/junkbox/openshift_and_cloudfoundry.html</id><summary type="html">&lt;p&gt;Stray Sheep - OpenShift and Cloud Foundry (again?)&lt;/p&gt;</summary><content type="html">&lt;p&gt;こんな記事が話題になっているようだ。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://techcrunch.com/2019/09/12/together-at-last-ibm-brings-cloud-foundry-to-red-hat-openshift/"&gt;IBM brings Cloud Foundry and Red Hat OpenShift together&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;オレPaaSはこれだぜ…といって始まったはずが、kubernetes 使うならOpenShift…に言うことが全然変わり、そしてまた Cloud Foundry ですか…迷える子羊さんだなー…というのが第一印象。&lt;/p&gt;
&lt;p&gt;とはいえ、API(と、それで制御されるざっくりした構造も…かな)だけ維持して、既存のk8sベースのインフラでCF用に作られたアプリケーションを収容できるようにするというのは自然な発想と言えるかなと思うのだが、この動きって最初から計画されていたのか、それとも Cloud Foundry に大々的に投資していたIBMの意向なのか、どちらなんでしょう？
あと、OpenShiftの場合、Kuryr とか使ってOpenStackと並べて動かすみたいな話もあるようだし、ただでさえ重量級な
OpenShiftがますます複雑化しそうな雰囲気なのがちょっと...(とおいめ&lt;/p&gt;
&lt;p&gt;余談だけど、OpenShift の歴史、特に v1 と v2 の頃ってどんなんだっけ？とググっていたらこんな記事も出てきた。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developer.ibm.com/blogs/a-brief-history-of-red-hat-openshift/"&gt;A brief history of Kubernetes, OpenShift, and IBM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;これって、8/1付の記事なのだけど、最初の記事が 9/11-12の Cloud Foundry Summit 2019での &lt;em&gt;IBM&lt;/em&gt; による発表の記事なので、情報は先に出ていた...ということですか。&lt;/p&gt;
&lt;p&gt;ところで、１つめの記事から、こんな記事もリンクされていた。&lt;/p&gt;
&lt;p&gt;[With its Kubernetes bet paying off, Cloud Foundry doubles down on developer experience]
(https://techcrunch.com/2019/09/09/with-its-kubernetes-bet-paying-off-cloud-foundry-double-down-on-developer-experience/)&lt;/p&gt;
&lt;p&gt;Fortune 500に載る会社の50%以上がCloud Foundry使っているって本当？…というのはおいておいて、
PivotalもCloud Foundryをk8sの上で動かすぜー…なんて言っていたのね。知りませんでした...orz&lt;/p&gt;
&lt;p&gt;&lt;a href="https://content.pivotal.io/announcements/pivotal-makes-kubernetes-easier-for-developers-and-operators"&gt;Pivotal Makes Kubernetes Easier for Developers and Operators&lt;/a&gt;&lt;/p&gt;</content><category term="tech"></category></entry><entry><title>[ja] On OpenSDS</title><link href="https://thatsdone.github.io/junkbox/opensds.html" rel="alternate"></link><published>2019-09-03T00:00:00+09:00</published><updated>2019-09-03T00:00:00+09:00</updated><author><name>thatsdone</name></author><id>tag:thatsdone.github.io,2019-09-03:/junkbox/opensds.html</id><summary type="html">&lt;p&gt;My recent OpenSDS related activities&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近は趣味と実益を兼ねて &lt;a href="https://opensds.io/"&gt;OpenSDS&lt;/a&gt; をいじっています。&lt;/p&gt;
&lt;p&gt;OpenSDS は、名前からして「また新しい Software Defined Storage の実装ですか？」と聞かれるのですが、実は複数のストレージコントローラをたばねるものです。
block storage から出発して、今はNFSのような file storage や Object Storage もサポートしています。&lt;/p&gt;
&lt;p&gt;その昔いじっていたOpenStackも sub project がたくさんありましたが、OpenSDS もだいぶにぎやかになってきました。&lt;/p&gt;
&lt;p&gt;この中では telemetry と anomaly-detection (仕事的には multi-cloudも) に興味があるのですが、まずは簡単に使えるようにしなくちゃね...ということで、
githubの私のページの activity を見てもらってもわかる通り、
opensds-installer をせっせと直しています。まずは ansible で、helm chart はその次ですかね...&lt;/p&gt;
&lt;p&gt;余談ですが、現在、OpenSDSの認証には OpenStack の Keystone が導入されています。
Keystone の場合、認証した結果に各種関連サービスの endpoint の一覧が返却される
という変わった(?)仕様なのですが、このあたり、どこまで integration するかは
まだやわらかいようです。&lt;/p&gt;</content><category term="tech"></category></entry></feed>